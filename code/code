from typing import Tuple, Any
import torch
import mat4py
import numpy as np
import torch.nn as nn
from sklearn.metrics.pairwise import cosine_similarity
import torch.nn.functional as F
import matplotlib
from torch import Tensor
matplotlib.use('TkAgg') 


def load_txt_file2ts(path, fType):
    return torch.from_numpy(np.loadtxt(path)).to(fType)


def tensor_shuffle(ts, dim=0):
    return ts[torch.randperm(ts.shape[dim])]


def update_ass(mi_dis_ass_ts, test_x_y):
    mi_dis_ass_ts_new = mi_dis_ass_ts.clone()
    for item in test_x_y:
        mi_dis_ass_ts_new[item[0]][item[1]] = 0
    return mi_dis_ass_ts_new


def acc_features(drug_micro_ass_ts_update, drug_sim_ts, micro_sim_ts, x, y):
    # (mi的相似度拼接mi与疾病的联系) 堆叠 (疾病与mi的联系拼接疾病的相似度_
    # drug_sim_ts, (1373, 1373); drug_micro_ass_ts_update, (1373, 173); micro_sim_ts, (173, 173);
    mat = torch.stack((torch.cat((drug_sim_ts[x, :], drug_micro_ass_ts_update[x, :]), dim=0),
                       torch.cat((drug_micro_ass_ts_update[:, y], micro_sim_ts[y, :]), dim=0)), dim=1)
    # mat, (通道数, 宽度, 长度), (1, 2, 1546);
    mat = mat.view(1, 2, -1)
    return mat


# @ [tensor([[1, 2], [3, 4]]), tensor([5, 6])] -->> tensor([[1, 2], [3, 4], [5, 6]])
def cat_ts(ls, dim=0):
    for i in range(len(ls)):
        ls[i] = [ls[i].unsqueeze(0), ls[i]][len(ls[i].shape) != 1]
    return torch.cat(ls, dim=0)


def mat2cosine_sim_mat(ts):
    return torch.from_numpy(cosine_similarity(ts))


def mat2cosine_sim_mat4NBig_datat(ts):
    return torch.cosine_similarity(ts.unsqueeze(1), ts.unsqueeze(0), dim=2, eps=1e-08)


def mat2cosine_sim_mat4big_data(ts):
    row_nums = ts.shape[0]
    mat = torch.zeros((row_nums, row_nums))
    for i in range(row_nums):
        mat[i] = torch.cosine_similarity(ts[i].unsqueeze(0), ts, dim=1, eps=1e-08)
    return mat


def build_feature_view(X: torch.Tensor,
                       P: torch.Tensor,
                       mask_ratio: float = 0.2
                       ) -> (torch.Tensor, torch.Tensor):
    col_norms = P.norm(p=2, dim=0)  # shape = [F]
    col_norms = col_norms / col_norms.sum()

    F = X.size(1)
    k = int(F * mask_ratio)
    _, idx_min = torch.topk(col_norms, k, largest=False)

    q = torch.ones(F, device=X.device)
    q[idx_min] = 0.0

    X_aug = X * q.unsqueeze(0)  # 广播到 [N, F]

    return X_aug, q


def node_view_consistency_loss(X_ori: torch.Tensor,
                               X_aug: torch.Tensor
                               ) -> torch.Tensor:
    diff = X_ori - X_aug
    loss = torch.norm(diff, p='fro') ** 2
    return loss


def build_structure_view(X: torch.Tensor,
                         P: torch.Tensor,
                         m: torch.Tensor
                         ) -> torch.Tensor:

    N = X.size(0)
    H = X @ P  # [N, N]

    H_i = H.unsqueeze(1)  # [N, 1, N]
    H_j = H.unsqueeze(0)  # [1, N, N]
    diff = torch.abs(H_i - H_j)  # [N, N, N]

    m_vec = m.view(-1, 1)  # [N, 1]
    scores = F.relu(diff @ m_vec)  # [N, N, 1]
    scores = scores.squeeze(-1)  # [N, N]

    exp_scores = torch.exp(scores)
    A_aug = exp_scores / exp_scores.sum(dim=1, keepdim=True)  # [N, N]

    return A_aug


def graph_structure_loss(A_aug: torch.Tensor,
                         P: torch.Tensor
                         ) -> torch.Tensor:
     N = A_aug.size(0)

    H_A = A_aug @ P  # [N, N]

    norms_sq = (H_A ** 2).sum(dim=1)  # [N]

    dist_sq = norms_sq.unsqueeze(1) + norms_sq.unsqueeze(0) - 2 * (H_A @ H_A.t())
    dist_sq = torch.clamp(dist_sq, min=0.0)
    D = torch.sqrt(dist_sq + 1e-8)  # [N, N]

    loss_struct = (D * A_aug).sum()

    loss_fro = torch.norm(A_aug, p='fro') ** 2

    return loss_struct + loss_fro


# ——— 对比损失 NT-Xent ———
def contrastive_loss(z1, z2, tau=0.1):
    """
    z1, z2: [N, D]，两种视图的节点嵌入
    """
    z1 = F.normalize(z1, dim=1)
    z2 = F.normalize(z2, dim=1)
    sim = torch.mm(z1, z2.t()) / tau  # [N, N]
    pos = torch.exp(torch.diag(sim))  # [N]
    denom = torch.exp(sim).sum(dim=1)  # [N]
    loss = -torch.log(pos / denom).mean()
    return loss


def extract_k_hop_subgraph(A: torch.Tensor,
                           seed_node: int,
                           k_hops: int = 1
                           ) -> (torch.Tensor, list):
       N = A.size(0)
    visited = {seed_node}
    frontier = {seed_node}
    for _ in range(k_hops):
        neighs = set()
        for u in frontier:
            nbrs = torch.nonzero(A[u] > 0, as_tuple=False).flatten().tolist()
            neighs.update(nbrs)
        frontier = neighs - visited
        visited |= neighs

    sub_nodes = sorted(visited)
    A_sub = A[sub_nodes][:, sub_nodes]
    return A_sub, sub_nodes


class SubgraphStructureExtractor(nn.Module):

    def __init__(self, in_dim: int, out_dim: int):
        super().__init__()
        self.W = nn.Parameter(torch.randn(in_dim, out_dim))

    def forward(self,
                A: torch.Tensor,
                X: torch.Tensor,
                seed_node: int,
                k_hops: int = 1
                ) -> torch.Tensor:
 
        A_sub, sub_nodes = extract_k_hop_subgraph(A, seed_node, k_hops)
        n_sub = A_sub.size(0)
        I_sub = torch.eye(n_sub, device=A.device)
        A_hat = A_sub + I_sub
        D_hat = A_hat.sum(dim=1)
        D_inv_sqrt = torch.diag(D_hat.pow(-0.5))
        norm_A = D_inv_sqrt @ A_hat @ D_inv_sqrt  # [n_sub, n_sub]
        H_sub = X[sub_nodes]  # [n_sub, in_dim]
        M = F.relu(norm_A @ H_sub @ self.W)  # [n_sub, out_dim]
        m_mean = M.mean(dim=0, keepdim=True)  # [1, out_dim]
        p = sub_nodes.index(seed_node)
        h_prev = H_sub[p: p + 1]  # [1, in_dim]
        h_self = h_prev @ self.W  # [1, out_dim]
        h_concat = torch.cat([h_self, m_mean], dim=1)  # [1, 2*out_dim]
        h_x_k = F.relu(h_concat)  # [1, 2*out_dim]

        return h_x_k


class MultiHeadStructureAttention(nn.Module):

    def __init__(self, d_model: int, n_heads: int):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads 


        self.Wq = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(n_heads)])
        self.Wk = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(n_heads)])
        self.Wv = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(n_heads)])
        self.out_proj = nn.Linear(d_model, d_model)

        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model)
        )

    def forward(self,
                h: torch.Tensor,
                subgraph_idxs: list[list[int]]
                ) -> torch.Tensor:

        N, _ = h.shape
        device = h.device


        SSA = torch.zeros_like(h, device=device)  # [N, d_model]
        for i in range(N):
            h_i = h[i]  # [d_model]
            neighbors = h[subgraph_idxs[i]]  # [n_i, d_model]
            head_outputs = []

            for head in range(self.n_heads):

                Qx = self.Wq[head](h_i)  # [d_k]
                Ku = self.Wk[head](neighbors)  # [n_i, d_k]


                # scores_j = exp( <Qx,Ku_j> / sqrt(d_k) )
                scores = torch.exp((Qx.unsqueeze(0) * Ku).sum(-1) / matplotlib.sqrt(self.d_k))  # [n_i]
                alpha = scores / scores.sum()  # softmax over neighbors


                Vu = self.Wv[head](neighbors)  # [n_i, d_k]

         
                head_out = (alpha.unsqueeze(-1) * Vu).sum(dim=0)  # [d_k]
                head_outputs.append(head_out)

  
            h_ssa_i = torch.cat(head_outputs, dim=-1)  # [d_model]
            SSA[i] = h_ssa_i

        x_prime = h + SSA / matplotlib.sqrt(self.d_model)

        x_prime = self.out_proj(x_prime)


        x_out = self.ffn(x_prime)  # [N, d_model]
        return x_out


class StructureTransformer(nn.Module):

    def __init__(self, d_model: int = 256, n_heads: int = 8, n_layers: int = 2):
        super().__init__()
        layers = []
        for _ in range(n_layers):
            layers.append(MultiHeadStructureAttention(d_model, n_heads))
        self.layers = nn.ModuleList(layers)

    def forward(self,
                h: torch.Tensor,
                subgraph_idxs: list[list[int]]
                ) -> torch.Tensor:
        x = h
        for layer in self.layers:
            x = layer(x, subgraph_idxs)
        return x


def intra_view_contrastive_loss(z1, z1_p, z2, z2_p, Nd, Nm, tau=0.5):
   
    device = z1.device
    N, d = z1.shape
    assert N == Nd + Nm, "总节点数应等于 Nd + Nm"

    z1 = F.normalize(z1, dim=1)  # [N, d]
    z1_p = F.normalize(z1_p, dim=1)
    z2 = F.normalize(z2, dim=1)
    z2_p = F.normalize(z2_p, dim=1)

    sim1 = torch.exp((z1 @ z1_p.t()) / tau)  # [N, N]
    sim2 = torch.exp((z2 @ z2_p.t()) / tau)

    idx = torch.arange(N, device=device)
    drug_idx = idx[:Nd]  # 0..Nd-1
    micro_idx = idx[Nd:]  # Nd..Nd+Nm-1

    def loss_for_group(sim):
        L1 = []
        # drugs
        for i in drug_idx:
            num = sim[i, i]  # 正样本对
            den = sim[i, drug_idx].sum()
            L1.append(-torch.log(num / den + 1e-12))
        # microbes
        for i in micro_idx:
            num = sim[i, i]
            den = sim[i, micro_idx].sum()
            L1.append(-torch.log(num / den + 1e-12))
        return torch.stack(L1).sum()

    loss1 = loss_for_group(sim1)
    loss2 = loss_for_group(sim2)

    loss = 0.5 * (loss1 / Nd + loss1 / Nm) + \
           0.5 * (loss2 / Nd + loss2 / Nm)

    return loss


def inter_view_contrastive_loss(z1, z1_p, z2, z2_p, Nd, Nm, tau=0.5, eps=1e-12):
    device = z1.device
    N, d = z1.shape

    z1 = F.normalize(z1, dim=1)
    z1_p = F.normalize(z1_p, dim=1)
    z2 = F.normalize(z2, dim=1)
    z2_p = F.normalize(z2_p, dim=1)
    #    sim1p2[i,j] = exp(⟨z1'_i, z2_j⟩/τ)
    sim1p2 = torch.exp((z1_p @ z2.t()) / tau)  # [N, N]
    #    sim12p[i,j] = exp(⟨z1_i, z2'_j⟩/τ)
    sim12p = torch.exp((z1 @ z2_p.t()) / tau)  # [N, N]
    idx = torch.arange(N, device=device)
    drug_idx = idx[:Nd]  # 0..Nd-1
    microbe_idx = idx[Nd:]  # Nd..Nd+Nm-1

    loss_drug = 0.0
    loss_micro = 0.0

    for i in drug_idx:
        # L2(di)
        num = sim1p2[i, i]
        denom = sim1p2[i, drug_idx].sum() + eps
        L2 = -torch.log(num / denom)

        # L2'(di)
        num_p = sim12p[i, i]
        denom_p = sim12p[i, drug_idx].sum() + eps
        L2_p = -torch.log(num_p / denom_p)

        loss_drug += (L2 + L2_p)
    for j in microbe_idx:
        # L2(mj)
        num = sim1p2[j, j]
        denom = sim1p2[j, microbe_idx].sum() + eps
        L2 = -torch.log(num / denom)

        # L2'(mj)
        num_p = sim12p[j, j]
        denom_p = sim12p[j, microbe_idx].sum() + eps
        L2_p = -torch.log(num_p / denom_p)

        loss_micro += (L2 + L2_p)

    loss = loss_drug / (2 * Nd) + \
           loss_micro / (2 * Nm)

    return loss


class FourViewAttention(nn.Module):

    def __init__(self, dim: int):
        super().__init__()
        # W_att: [dim, dim], b: [dim], q: [dim]
        self.W_att = nn.Parameter(torch.randn(dim, dim))
        self.b = nn.Parameter(torch.zeros(dim))
        self.q = nn.Parameter(torch.randn(dim))

    def forward(self,
                z1: torch.Tensor,
                z1_p: torch.Tensor,
                z2: torch.Tensor,
                z2_p: torch.Tensor
                ) -> tuple[Any, Tensor]:
    
        Z = torch.stack([z1, z1_p, z2, z2_p], dim=0)  # [4, d]

        M = torch.tanh(F.linear(Z, self.W_att, self.b))  # [4, d]

        omega = M @ self.q  # [4]

        alpha = F.softmax(omega, dim=0)  # [4]

        weighted = alpha.unsqueeze(1) * Z  # [4, d]

        z_out = weighted.view(-1)  # [4*d]

        return z_out, alpha

class CNN(nn.Module):

    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            # batch_size, 1, 2, 1546
            nn.Conv2d(1, 16, kernel_size=(2, 1), stride=1, padding=1),
            # batch_size, 16, 3, 1548
            nn.Conv2d(16, 16, kernel_size=(3, 1), stride=1, padding=0),
            # batch_size, 16, 1, 1548
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(1, 4), stride=4)
            # batch_size, 16, 1, 387
        )
        self.fc = nn.Sequential(
            nn.Linear(16 * 1 * 387, 2048),
            nn.BatchNorm1d(2048),
            # nn.ReLU(),
            # nn.Linear(2048, 256),
            # nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(2048, 2))

    def forward(self, x):
        # x, (batch_size, 1, 2, 1546)
        o1 = self.conv1(x)
        # o2= self.conv2(o1)
        return self.fc(o1.reshape(o1.shape[0], -1))

if __name__ == '__main__':
    micr_drug_ass_mat_path = 'net1.mat'
    drug_feature_path = 'drug_features.txt'
    micr_feature_path = 'microbe_features.txt'
    # 将mi与dis的关联矩阵读入 并转tensor类型
    # drug_micro_ass_ts, (1373, 173)
    drug_micro_ass_ts = torch.tensor(mat4py.loadmat(micr_drug_ass_mat_path)['interaction'])
    # drug_sim_ts, (1373, 1373)
    drug_sim_mat_ts = torch.from_numpy(np.loadtxt('drugsimilarity.txt')).to(torch.float)
    # micro_sim_ts, (173, 173)
    micr_sim_mat_ts = torch.from_numpy(np.loadtxt('microbe_microbe_similarity.txt')).to(torch.float)
    # 检索非零值，返回其索引 返回索引个数 7908. mi_dis_ass_nonzero_index, like [[0, 0],[0, 1],[0, 2]...], (7908, 2)
    drug_micro_ass_nonzero_index = torch.nonzero(drug_micro_ass_ts)
    # 检索零值 (262505, 2)
    drug_micro_ass_zero_index = torch.nonzero(drug_micro_ass_ts == 0)
    # shuffle mi_dis_ass_nonzero_len, 7908
    drug_micro_ass_nonzero_index_shuffle = tensor_shuffle(drug_micro_ass_nonzero_index)
    drug_micro_ass_nonzero_len = len(drug_micro_ass_nonzero_index)
    # mi_dis_ass_zero_len, 262505
    drug_micro_ass_zero_index_shuffle = tensor_shuffle(drug_micro_ass_zero_index)
    # 对反例索引进行划分 得到一份与正例同等大小的数据、一份剩余反例索引数据 mi_dis_ass_zero_index_shuffle1, (7908, 2); mi_dis_ass_zero_index_shuffle2, (254597, 2)
    drug_micro_ass_zero_index_shuffle1, drug_micro_ass_zero_index_shuffle2 = drug_micro_ass_zero_index_shuffle[
                                                                             0: drug_micro_ass_nonzero_len], drug_micro_ass_zero_index_shuffle[
                                                                                                             drug_micro_ass_nonzero_len:]
    # 将三份数据（正例索引、反例索引、剩余反例索引）拼接得到完整的索引集合 dataset_positive_negitive_balpn_neg, (2470+ 2470+ 237529, 2)
    dataset_positive_negitive_balpn_neg = torch.cat((tensor_shuffle(
        torch.cat((drug_micro_ass_nonzero_index_shuffle, drug_micro_ass_zero_index_shuffle1))),
                                                     drug_micro_ass_zero_index_shuffle2))
    # 获得索引数据集对应的标签
    dataset_positive_negitive_balpn_neg_labels = torch.tensor(
        [drug_micro_ass_ts[item[0]][item[1]] for item in dataset_positive_negitive_balpn_neg])
    # 标签转tensor类型
    dataset_positive_negitive_balpn_neg_labels = dataset_positive_negitive_balpn_neg_labels.to(torch.long)
    n_d, n_m = drug_sim_mat_ts.size(0), micr_sim_mat_ts.size(0)
    # 已知正例数
    num_pos = 2470

    # 计算要选取的样本总数（正例 + 等量负例）
    num_select = num_pos * 2  # 4940

    # 切片前 num_select 条对索引
    selected_pairs = dataset_positive_negitive_balpn_neg[:num_select]  # shape [4940, 2]
    selected_labels = dataset_positive_negitive_balpn_neg_labels[:num_select]  # shape [4940]
